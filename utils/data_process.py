import os
from tqdm import tqdm
import random
import json
import numpy as np
from data.LetorDataset import LetorDataset


###################################################
#################### LDS - #R=1 ###################
# data process for label distribution skewness (#R=1) - Type 2
###################################################

def generate_label_distribution_skew_for_R1(dataset, n_folds, n_labels):
    for fold in range(n_folds):
        fold_id = fold + 1
        path = f"../datasets/{dataset}/Fold{fold_id}/train.txt"
        lines_0 = ""
        lines_1 = ""
        lines_2 = ""
        lines_3 = ""
        lines_4 = ""
        with open(path, "r") as f:
            for line in f:
                cols = line.strip().split()
                query = cols[1].split(":")[1]
                relevance = float(cols[0]) # Sometimes the relevance label can be a float
                if relevance.is_integer():
                    relevance = int(relevance) # But if it is indeed an int, cast it into one.

                if relevance == 0:
                    lines_0 += line
                elif relevance == 1:
                    lines_1 += line
                elif relevance == 2:
                    lines_2 += line
                elif relevance == 3:
                    lines_3 += line
                elif relevance == 4:
                    lines_4 += line

        new_data = [lines_0, lines_1, lines_2, lines_3, lines_4]

        for label in range(n_labels):
            save_path = f"../datasets/{dataset}/LDS/Fold{fold_id}/label_{label}/train.txt"

            if not os.path.exists(os.path.dirname(save_path)):
                os.makedirs(os.path.dirname(save_path))

            with open(save_path, 'w') as f:
                f.write(new_data[label])



###################################################
#################### LDS2 - V1 ####################
# data process for label distribution skewness (#R=2) Version 1 - Type 2
# each clients have data with two labels with overlapped dataset (query, doc and label).
# (generated by seperate all query-doc pairs by 10 relevance label combination).
###################################################

def generate_label_distribution_skew_for_R2_V1(dataset, n_folds, n_labels=10):
    for fold in tqdm(range(n_folds)):
        fold_id = fold + 1
        path = f"../datasets/{dataset}/Fold{fold_id}/train.txt"
        lines_0 = ""
        lines_1 = ""
        lines_2 = ""
        lines_3 = ""
        lines_4 = ""
        lines_5 = ""
        lines_6 = ""
        lines_7 = ""
        lines_8 = ""
        lines_9 = ""
        with open(path, "r") as f:
            for line in f:
                cols = line.strip().split()
                query = cols[1].split(":")[1]
                relevance = float(cols[0]) # Sometimes the relevance label can be a float
                if relevance.is_integer():
                    relevance = int(relevance) # But if it is indeed an int, cast it into one.

                if relevance == 0 or relevance == 1:
                    lines_0 += line
                if relevance == 0 or relevance == 2:
                    lines_1 += line
                if relevance == 0 or relevance == 3:
                    lines_2 += line
                if relevance == 0 or relevance == 4:
                    lines_3 += line
                if relevance == 1 or relevance == 2:
                    lines_4 += line
                if relevance == 1 or relevance == 3:
                    lines_5 += line
                if relevance == 1 or relevance == 4:
                    lines_6 += line
                if relevance == 2 or relevance == 3:
                    lines_7 += line
                if relevance == 2 or relevance == 4:
                    lines_8 += line
                if relevance == 3 or relevance == 4:
                    lines_9 += line

        new_data = [lines_0, lines_1, lines_2, lines_3, lines_4, lines_5, lines_6, lines_7, lines_8, lines_9]
        # print("length of new_data:", len(lines_0))
        # print("type of new_data:", type(new_data[0]))
        # print(new_data[0][0])

        for label in range(n_labels):
            save_path = f"../datasets/{dataset}/LDS2_V1/Fold{fold_id}/label_{label}/train.txt"

            if not os.path.exists(os.path.dirname(save_path)):
                os.makedirs(os.path.dirname(save_path))

            with open(save_path, 'w') as f:
                f.write(new_data[label])



###################################################
#################### LDS2 - V2 ####################
# data process for label distribution skewness (#R=2) Version 2 - Type 2
# each clients have data with two labels without overlapped dataset (query, doc and label).
# (generated by ...
# step 1: randomly assign all queries to 10 clients and randomly assign one relevance label combination C(5, 2) = 10 to each client,
# step 2: on each client, only keep the query-doc pairs with specific relevance label combination assigned to each client).
###################################################

def generate_label_distribution_skew_for_R2_V2(dataset, n_folds, n_features, data_norm, n_labels=10, seed=1):
    for fold in tqdm(range(n_folds)):
        fold_id = fold + 1
        path = f"../datasets/{dataset}/Fold{fold_id}/train.txt"

        train_0 = LetorDataset(path, feature_size=n_features, query_level_norm=data_norm, cache_root="../datasets/cache", abs_path=False)

        query_set = train_0.get_all_querys()
        random.Random(seed).shuffle(query_set)
        query_set_split = np.split(query_set, 10)

        label_set = [[0, 1], [0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 3], [2, 4], [3, 4]]

        for label in range(n_labels):
            queries = query_set_split[label]
            labels = label_set[label]
            lines = ""
            with open(path, "r") as f:
                for line in f:
                    cols = line.strip().split()
                    query = cols[1].split(":")[1]
                    relevance = float(cols[0]) # Sometimes the relevance label can be a float
                    if relevance.is_integer():
                        relevance = int(relevance) # But if it is indeed an int, cast it into one.

                    if (query in queries) and (relevance in labels):
                        lines += line

            save_path = f"../datasets/{dataset}/LDS2_V2/Fold{fold_id}/label_{label}/train.txt"

            if not os.path.exists(os.path.dirname(save_path)):
                os.makedirs(os.path.dirname(save_path))

            with open(save_path, 'w') as f:
                f.write(lines)



###################################################
############ LDS1 - V2 (data sharing) #############
# implementation of data-sharing strategy for label distribution skewness (#R=1)
###################################################

def data_sharing_with_label_distribution_skew_for_R1(dataset, n_folds, n_features, data_norm, n_labels, seed=1):
    for fold in tqdm(range(n_folds)):
        fold_id = fold + 1
        path = f"../datasets/{dataset}/Fold{fold_id}/train.txt"

        train_0 = LetorDataset(path, feature_size=n_features, query_level_norm=data_norm, cache_root="../datasets/cache", abs_path=False)

        query_set = train_0.get_all_querys()
        random_state = np.random.RandomState(seed)
        n_query = 600 # select 10% of data - change the value to adjust your need
        index = random_state.randint(len(query_set), size=n_query)
        query_selected = []
        for i in range(n_query):
            id = index[i]
            qid = query_set[id]
            query_selected.append(qid)

        lines_selected = ""
        with open(path, "r") as f:
            for line in f:
                cols = line.strip().split()
                query = cols[1].split(":")[1]

                if query in query_selected:
                    lines_selected += line

        for label in range(n_labels):
            noniid_path = f"../datasets/{dataset}/LDS/Fold{fold_id}/label_{label}/train.txt"
            lines_origin = ""
            with open(noniid_path, "r") as f:
                for line in f:
                    cols = line.strip().split()
                    query = cols[1].split(":")[1]

                    if query not in query_selected:
                        lines_origin += line

            lines_new = lines_origin + lines_selected
            save_path = f"../datasets/{dataset}/LDS1_datasharing/Fold{fold_id}/label_{label}/train.txt"

            if not os.path.exists(os.path.dirname(save_path)):
                os.makedirs(os.path.dirname(save_path))

            with open(save_path, 'w') as f:
                f.write(lines_new)



if __name__ == "__main__":
    # configuration
    dataset = "Istella-s" # "MQ2007" "MSLR10K" "Yahoo" "Istella-s"

    if dataset == "MQ2007":
        n_folds = 5
        n_features = 46
        data_norm = False
        n_labels = 3
    elif dataset == "MQ2008":
        n_folds = 5
        n_features = 46
        data_norm = False
        n_labels = 3
    elif dataset == "MSLR10K":
        n_folds = 5
        n_features = 136
        data_norm = True
        n_labels = 5
    elif dataset == "Yahoo":
        n_folds = 1
        n_features = 700
        data_norm = True
        n_labels = 5
    elif dataset == "Istella-s":
        n_folds = 1
        n_features = 220
        data_norm = True
        n_labels = 5

    # # implementation - choose the function that you want to implement
    # generate_label_distribution_skew_for_R1(dataset, n_folds, n_labels)
    # generate_label_distribution_skew_for_R2_V1(dataset, n_folds)
    # generate_label_distribution_skew_for_R2_V2(dataset, n_folds, n_features, data_norm)
    # data_sharing_with_label_distribution_skew_for_R1(dataset, n_folds, n_features, data_norm, n_labels)

    # # testing - check the length of query_set after partition
    # for fold in range(n_folds):
    #     fold_id = fold + 1
    #     for label in range(n_labels):
    #         train_0 = LetorDataset(f"../datasets/{dataset}/LDS/Fold{fold_id}/label_{label}/train.txt",
    #                                 n_features, query_level_norm=data_norm,
    #                                 cache_root="../datasets/cache",
    #                                 abs_path=False)
    #         query_set = train_0.get_all_querys()
    #         print("folder:", fold_id, "label:", label, "length of query_set:", np.shape(query_set))
    #
    #         for rel in range(n_labels):
    #             a = 0
    #             for query in query_set:
    #                 a += train_0.get_all_relevance_label_by_query(query).count(rel)
    #             print("label:", rel, a)
